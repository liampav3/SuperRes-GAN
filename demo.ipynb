{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project implements a generative adversarial network for the single-image super-resolution task. The implementation is based on the Super-Resolution GAN (SR-GAN) [1] and Enhanced Super-Resolution GAN (ESR-GAN) [2] papers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries, downloading dataset, and preparing dataset directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image, write_png\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run this block if working in Google Colab. Downloading repo into Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Binary output can mess up your terminal. Use \"--output -\" to tell \n",
      "Warning: curl to output it to your terminal anyway, or consider \"--output \n",
      "Warning: <FILE>\" to save to a file.\n",
      "tar: Error opening archive: Failed to open 'BSDS300-images.tgz'\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/liampav3/SuperRes-GAN\n",
    "!cd SuperRes-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we train and test our model on the BSDS5300 dataset, a high-resolution segmentation dataset with a variety of subjects and scenes. The dataset contains 200 training images and 100 test images.\n",
    "\n",
    "Note: Downloading the dataset is only required to run the training and quantitative results sections of this notebook. The qualitative results can be run with the incldued weights and image files in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/BSDS300-images.tgz\n",
    "!mkdir -p data\n",
    "!tar -xzf BSDS300-images.tgz -C data\n",
    "!mkdir -p data/BSDS300/images/test/cls\n",
    "!mkdir -p data/BSDS300/images/train/cls\n",
    "!mv data/BSDS300/images/train/*.jpg data/BSDS300/images/train/cls\n",
    "!mv data/BSDS300/images/test/*.jpg data/BSDS300/images/test/cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this section is computationally demanding and optional. You can instead opt to use the provided pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we apply a downscaling operation to the BSDS300 test dataset and test the model's ability to restore the images to their original resolution. We utilize bicubic interpolation upscaling as a baseline for comparison.\n",
    "\n",
    "Note: The quantitative results require downloading the dataset but the qualitative results can be run with the image files provided in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment quantatively compares the outputs of our method and the baseline on the peak signal-to-noise (PSNR) metric. As noted in the SR-GAN paper, this metric is somewhat flawed because it rewards heavily-smoothing outputs, which can destroy fine-grained textures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in trained generator\n",
    "gan_model = torch.load('weights/generator_final.torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in BSDS300 test set\n",
    "#Images are cropped to 300x300 to deal with differences in orientation\n",
    "test_ds = ImageFolder('BSDS300/images/test', transform=transforms.Compose([transforms.RandomCrop(300), transforms.ToTensor()]))\n",
    "test_dl = DataLoader(test_ds, batch_size=16)\n",
    "\n",
    "#Main quantitative evaluation loop\n",
    "psnr_gan = 0\n",
    "psnr_bc = 0\n",
    "for i, batch in enumerate(test_dl):\n",
    "    high_imgs = batch[0]\n",
    "    #Downsampling reference images\n",
    "    low_imgs = interpolate(high_imgs, scale_factor=1/4, mode='bicubic')\n",
    "\n",
    "    #Generating super-resolution images with bicubic and GAN model\n",
    "    bc_imgs = interpolate(low_imgs, scale_factor=4, mode='nearest')\n",
    "    gan_imgs = gan_model(low_imgs)\n",
    "    \n",
    "    #Computing MSE for GAN and bicubic\n",
    "    bc_mse = torch.mean((bc_imgs-high_imgs)**2, dim=(1,2,3))\n",
    "    gan_mse = torch.mean((gan_imgs-high_imgs)**2, dim=(1,2,3))\n",
    "    \n",
    "    #Computing GAN and bicubic PSNR for this batch and adding to running total\n",
    "    psnr_gan += torch.sum(20*torch.log10(1/torch.sqrt(gan_mse)))\n",
    "    psnr_bc += torch.sum(20*torch.log10(1/torch.sqrt(bc_mse)))\n",
    "#Taking average PSNR over dataset\n",
    "psnr_gan /= len(test_ds)\n",
    "psnr_bc /= len(test_ds)\n",
    "\n",
    "print(f'PSNR GAN: {psnr_gan}')\n",
    "print(f'PSNR Bicubic: {psnr_bc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to SR-GAN, our model is slightly outperformed by bicubic interpolation on the PSNR metric. This is expected since the model is trained to prioritize texture/feature sharpness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qualitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next experiment, we demonstrate the superior visual quality of our model's outputs by qualitative assesment. To do this, we visually inspect the outputs of our model and the baseline on a few select test images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visualizes the bicubic interpolation and GAN outputs of a single test image\n",
    "Plot shows the full image outputs of both methods, as well as a selected zoom region to inspect finer textures/details\n",
    "Original image is also visualzied for comparison\n",
    "'''\n",
    "def visualize_outputs(img_file, zoom_range):\n",
    "    img = transforms.ToTensor(read_image(img_file))\n",
    "\n",
    "    #Downscaling img\n",
    "    img = img.unsqueeze(dim=0)\n",
    "    low_img = interpolate(img, scale_factor=1/4, mode='bicubic')\n",
    "\n",
    "    #Upscaling with Bicubic and GAN\n",
    "    bc_img = interpolate(low_img, scale_factor=4, mode='nearest')\n",
    "    gan_img = gan_model(low_img)\n",
    "\n",
    "    _f, axs = plt.subplots(3, 2, figsize=(36, 24))\n",
    "\n",
    "    imgs_to_plot = [img, bc_img, gan_img]\n",
    "    for idx, pimg in imgs_to_plot:\n",
    "        #Display full image\n",
    "        axs[idx, 0].imshow(pimg)\n",
    "        #Display\n",
    "        axs[idx, 1].imshow(pimg[:, zoom_range[0,0]:zoom_range[0, 1], zoom_range[1,0]:zoom_range[1,1]])\n",
    "    \n",
    "    names = ['Original', 'Bicubic', 'GAN']\n",
    "    for idx, name in enumerate(names):\n",
    "        axs[idx, 0].set_title(name)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_outputs('qual_imgs/tiger.png', [[800, 1000], [700, 900]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO ADD MORE IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Ledig, Christian, et al. \"Photo-realistic single image super-resolution using a generative adversarial network.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
    "\n",
    "[2] Wang, Xintao, et al. \"Esrgan: Enhanced super-resolution generative adversarial networks.\" Proceedings of the European conference on computer vision (ECCV) workshops. 2018. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
